\chapter{Implementation}
\label{Chapter4}
% Main chapter title
\lhead{Chapter 4. \emph{Implementation}}

This Chapter presents the implementation overview of our approach. Section \ref{selectingCandidates} presents an outline about selecting suitable Selenium test-suites for our research. Section \ref{implementationsetup} discusses the implementation and infrastructural setup necessary to evaluate the robustness of Selenium test-suites. 

% Section \ref{infra} does infra, Section \ref{stateModelExtraction} extracts state models using webmate. \ref{selectingCandidates} selects candidates, Section bleh}

% All subsections start from here onwards:
\section{Web Applications and Selenium Test-suites}
\label{selectingCandidates}
In order to evaluate the robustness of Selenium tests, our objective was to empirically investigate different open-source web applications that have publicly available Selenium test-suites. The idea was that open-source Selenium tests generally have multiple contributors, which will provide a richer and unbiased data set as compared to tests written by a single individual. Adhering to this idea, the first step is to select suitable test candidates that satisfy a certain set of requirements for this research. 

According to the approach presented in Section \ref{robustnessOfSeleniumTests}, the first requirement for the test candidates is to have a sufficiently long development history, preferably with major versions and minor revisions. The reason for selecting open-source applications is that the older versions/releases of their applications are usually available for public usage. Availability of different releases is essential for this thesis since we are evaluating the robustness over the version history of web applications. In case there is no clear distinction made between major and minor releases for certain applications, we plan to resort to other options for identifying different releases, such as checking out \texttt{tag}s or commits from version control repositories. 

The second requirement for candidates fulfilling above requirements is to have publicly available Selenium test-suites with a minimum of ten Selenium tests. A test number below that would not be sufficient for this analysis. As mentioned in Chapter \ref{Chapter2}, this thesis considers Selenium \texttt{webdriver} based tests. These tests can be run using the \texttt{RemoteWebdriver} implementation and are to be integrated with the tool \texttt{webmate} as we will see in Section \ref{stateModelExtraction}. Additionally, preference is given to the test-suites which are being developed in parallel to their respective web applications, to evaluate the approach presented in Section \ref{locatorMaintenance}. Moreover, our implementation considers tests available in all supported languages, such as Java, Python, etc. 

To identify whether a given test-suite is suitable for our analysis, we locally deploy the AUT, execute Selenium tests on it and record the tests' outcome. Some tests are designed specifically for an organization's internal testing servers or may require certain configurations not available for public use. In other words, not all tests from a test-suite are expected to work on locally deployed instances. In such cases, we record the number of tests that are suitable for running locally and consider them for our analysis. 


% \subsubsection{Candidate Selection}
\section{Implementation setup}
\label{implementationsetup}
As we have seen in Chapter \ref{Chapter1}, Figure \ref{fig:thesisoverview2} depicts the setup required for the robustness analysis of selected test candidates. The following sections provide a brief overview of different steps of this implementation setup.  
\begin{figure}[h]
% [htbp]
	\centering	\includegraphics[width=\textwidth]{./Figures/thesisoverviewsmall.jpg}
%     [width=5cm, height=3cm]
% 		\rule{35em}{0.5pt}
	\caption{Setup required for the robustness analysis}
	\label{fig:thesisoverview2}
\end{figure} 

\subsection{Automated Application Deployment}
\label{sec:autoDeployment}



% \section{Web Application Deployment}

To measure the robustness of Selenium tests across different versions of AUT, the first step of the implementation setup is to make these versions accessible for the testing setup as shown in Figure \ref{fig:thesisoverview2}. It is possible to accomplish this by installing and deploying these versions locally (without relying on external application server provider). Generally, most of the open-source application vendors make older versions of their applications available for public usage. The most common ways to release such web applications are by releasing packaged versions ready to deploy, such as \texttt{war} (Web application ARchive) files in case of Java applications, by checking out the code from version control repositories such as git, subversion, mercurial, etc.

\begin{figure}[h]
\makeatletter 
% \renewcommand{\thefigure}{\@arabic\c@figure}
\makeatother
% \setcounter{figure}{0}
    \centering
  \includegraphics[width=5.4in,height=2.6in]{./Figures/Deployment_Process_2.jpg}
  \captionsetup{justification=raggedright,
singlelinecheck=false
}
  \caption{Automatic deployment workflow to deploy multiple instances (versions) of AUT in parallel}
  \label{fig:deployment} 
\end{figure}

Depending upon the type of a web application, the frameworks and programming languages used to create these applications, different set of application servers can be used for deploying the AUT. Java based packaged applications such as \texttt{war} archives can be deployed in Java containers such as Tomcat\footnote{\url{http://tomcat.apache.org/}}. Similarly, PHP based applications can be hosted inside Apache httpd\footnote{\url{https://httpd.apache.org/}} servers. 

The AUT can be installed as per the installation instructions provided by the application vendors. The installation setup includes steps such as downloading the application source code or packaged binaries, creating databases and other configurations, etc. Certain Selenium tests require the AUT to have some pre-configured features enabled, such as an \textit{administrative} user account setup, which can also be accomplished in the installation phase. 

In order to spare the manual effort of installing multiple versions of AUT and to be able to reproduce our results, we have automated the deployment work-flow through automated deployment scripts. In order to speed up the testing process, we deploy multiple versions of the AUT in parallel.  The applications are sand-boxed inside a secure Linux based virtual machine for security reasons. Figure \ref{fig:deployment} depicts the overview of remote automatic deployment process; three versions of the AUT are running simultaneously against their individual database instances.

% \subsection{Automatic application deployment}
\subsection{Behavioral State Models}
\label{stateModelExtraction}
Subsequently, the next steps (see Figure \ref{fig:thesisoverview2}) are to execute the Selenium tests on the AUT and extract the behavioral state models. In order to measure the \textit{robustness grade} of a test across two different versions, it is essential to verify whether the test covers same functional states across both versions. For this purpose, the behavioral state models are compared to check if same states of the AUT are reached by the test. 

\begin{figure}[h]
\makeatletter 
% \renewcommand{\thefigure}{\@arabic\c@figure}
\makeatother
    \centering
  \includegraphics[width=5.5in,height=4.5in]{./Figures/WebMate_state_extraction.png}
% \begin{flushleft}

\caption{Extraction of behavioral state models using \texttt{webmate}}
% \end{flushleft}
 \label{fig:webmateExtraction} 
\end{figure}

As elaborated in Section \ref{sec:WebMate}, \texttt{webmate} can leverage existing Selenium tests to extract a behavioral state model (usage-model) of the AUT. Figure \ref{fig:webmateExtraction} depicts the process of extracting behavioral state models by integrating the test scripts with \texttt{webmate}. Two different versions of an application are deployed in parallel as per the process as described in Section \ref{sec:autoDeployment}. For security reasons, Selenium tests are integrated with \texttt{webmate} inside a virtual machine that hosts a Selenium hub. With this setup, \texttt{webmate} supports multiple testing instances by using the \texttt{RemoteWebDriver} capabilities for communicating with the Selenium test scripts. The Selenium test scripts and the base URL of the application are given as input to \texttt{webmate}, as it can simultaneously execute the test scripts on different versions of the AUT to generate the behavioral state models. For each state of the behavioral state model, \texttt{webmate} is able to record its visual (GUI) state, in the form of a screen shot of the AUT. 

Essentially, the states of the state model represent the DOM state of the AUT, and the transitions represent the \textit{actions} executed by the test on the AUT. These actions are recorded in the form of a tuple \texttt{[session-Id, action \{key="value"\}]}. Every state model is uniquely identified by the browser \texttt{session-Id} associated with each test session, whereas an `action' denotes a user-action emulated by the test on the AUT.  

As an example, let us revisit the \texttt{loginTest} we have seen in Listing \ref{code1}. When the \texttt{loginTest} is executed with \texttt{webmate}, a state model resembling the structure in Listing \ref{statemodellisting} is extracted. Each action executed by the test, e.g. a \texttt{[findElement \{using="id", value="uname"\}]} locator request, can be identified by the unique \texttt{session-Id} of the test. 
At this point it is also important to mention that the behavioral state model can only capture \texttt{wait} commands which have a \textit{certain} measurable duration, such as \texttt{implicitlyWait} conditions. Commands with uncertain duration, such as \texttt{wait.until} conditions cannot be captured within the model and therefore it is not possible to evaluate them in this thesis. Chapter \ref{Chapter6} discusses this possible threat to validity. 

\begin{center}
\begin{scriptsize}
\centering
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  keepspaces=true,
%   frame=none,
}
% \verb|basicstyle=\ttfamily, columns=fullflexible, keepspaces=true|
  
\begin{lstlisting}[caption=Extracted behavioral state model for \texttt{loginTest},label=statemodellisting]
 actions : { 
[session-Id, get {url="http://application-under-test.com"}]
},
 actions : { 
[session-Id, findElement {using="id", value="uname"}]
},
 actions : { 
[session-Id, sendKeysToElement {id="1", value=["user"]}],
[session-Id, findElement {using="id", value="pwd"}]
},
 actions : { 
[session-Id, sendKeysToElement {id="2", value=["passwd"]}],
[session-Id, findElement {using="link text", value="Login"}]
},
 actions : { 
[session-Id, clickElement {id="3"}]
}
\end{lstlisting}
\end{scriptsize} 
\end{center}

% level differences between the reference version and the subsequent versions (e.g. minor revisions) of the AUT. 
% \subsection{Comparing Behavioral State Models}
\subsection{Implementation Process}
\label{toolimplementation}
% This section presents an overview of the implementation process. 

\textbf{To answer RQ1}, it is necessary to assess if a test can cover the same functionality as the AUT evolves. A test should not be broken by mere look-and-feel related changes in the AUT, such as moving a button to a geometrically different position. However, if a test reaches different application state, chances are the test is broken. This is where comparison of state models comes in -- to check whether a test covers the same states for different versions of the AUT. As mentioned in Section \ref{robustnessOfSeleniumTests}, the approach is to measure the  \textit{robustness grade} ($R_{T_{V_{0}V_{1}}}$) of the test \textit{T} for a newer version \textit{$V_{1}$} compared against the reference version \textit{$V_{0}$}, as following:
\begin{enumerate}
\item Initially, the test \textit{T} is executed on reference version \textit{$V_{0}$} and the behavioral state model \textit{$M_{0}$} for it is extracted and stored as shown in Listing \ref{stateModelExtraction}. 
\item Thereafter, the test \textit{T} is executed on newer version \textit{$V_{1}$} in similar manner and the state model \textit{$M_{1}$} for it is recorded as well.

\item In the next step, for determining the \textit{robustness grade} we perform an equivalence check of the states of the behavioral models \textit{$M_{0}$} and \textit{$M_{1}$} for versions \textit{$V_{0}$} and \textit{$V_{1}$} respectively.  
Note that the state equivalence check also includes the visual (GUI) state recorded by \texttt{webmate}.
% The presented tool parses the extracted state models and compares the states reached by them. For comparing the GUI states, \textit{ImageMagick Convert}\footnote{http://www.imagemagick.org/script/convert.php} -- a command-line tool, has been used in combination of the presented tool. The GUI states are compared by the juxtaposition of two screen-shots. 

\item If both state models are equivalent then the test covers the same functionality across  \textit{$V_{0}$} and \textit{$V_{1}$}, and it is, therefore, considered as robust. If the states of the behavioral models do not match, the test is marked as non-robust. For non-robust tests, the test execution results such as \textit{stack-traces} are examined to identify the reasons behind non-robust tests, such as \texttt{webdriver} exceptions \cite{SeleniumExceptions} and errors reported by the test. 

\item In this manner, the robustness grade for each test in the test-suite (\textit{$T_1$, $T_2$, ...$T_n$ $\in$ TS}) is calculated. Afterwards, using these results the robustness grade ($R_{TS_{V_{0}V_{1}}}$) of the test-suite is calculated as well.

\end{enumerate}

\textbf{To answer RQ2}, the approach is to examine the influence of the \textit{robustness metrics} (proposed in Section \ref{robfactors}) on the \textit{ground truth} defined in Section \ref{robustnessOfSeleniumTests}. As mentioned earlier, the state model captures the `actions' executed by Selenium tests on the AUT. The approach for extracting the robustness metrics and the ground truth with the help of the state models is as follows: 

\begin{enumerate}

\item A test-suite \textit{TS} with \textit{n} tests \textit{$T_1$, $T_2$, ...$T_n$ $\in$ TS} is executed on reference version \textit{$V_{0}$} and the behavioral state model is captured. 

\item After generating the behavioral model, it is then parsed to extract the robustness metrics from the transitions of the state model, i.e. the `actions'.

\item From the results obtained in the process of answering RQ1, the \textit{ground truth} is calculated for each test  according to Definition \ref{robustness-ground-truth}.

\item The robustness metrics and the ground truth are represented as a feature matrix extended with a solution vector, as detailed in Section \ref{sec:Statistical}.

\item Thereafter, statistical analysis is performed on the generated feature matrix -- for estimating the correlation coefficient between the features and the solution vector. To identify the most influential features, the \textit{lasso} regression method is applied, as described in Section \ref{sec:Statistical}.
\end{enumerate}



\textbf{To answer RQ3}, our approach is to measure the \textit{maintenance metrics} in terms of the number of GUI element locators as well as \texttt{wait} commands repaired between two successive revisions of the AUT, as described in Section \ref{locatorMaintenance}. A repairing procedure usually involves changing, adding or removing a certain element locator or \texttt{wait} condition. 

In many software development environments, the Selenium tests-suite is maintained in parallel to the development of the web applications. In other words, when a test-suite is repaired because of the changes in the AUT, it is often updated following these changes in the AUT. In this manner, different versions (\textit{$V_1, ...V_n$}) of the application can have corresponding versions (e.g. \texttt{commits} to the version control repository) of Selenium test-suites (\textit{$TS_1, ...TS_n$}). It is also possible that a test-suite is not repaired after a release because all the tests were robust. In such cases, we consider that a given version $V_1$ does not have a corresponding test-suite version $TS_1$. This indicates that there was no change in the maintenance metrics for this version of the test-suite.

As mentioned earlier, a behavioral state model captures the actions executed by the Selenium tests. The proposed maintenance metrics are measured in terms of a change-set between two versions of the test-suite. The idea is to measure what metrics have been repaired between two test-suite revisions through the comparison of the state models. A ``repair'' procedure indicates changing, adding or removing a maintenance metric. As an example, by comparing the behavioral models for two versions of the \texttt{loginTest}, we can detect the changes GUI element locator from \texttt{[findElement \{using="link text", value="Login"\}]} to \texttt{[findElement \{using="link text", value="Sign In"\}]}. The change-set in this case would be = \texttt{\{changed locator:"link text", value:"Login"\}}.
The approach for obtaining this change-set is as following:

\begin{enumerate}
\item To measure the proposed maintenance metrics, each version $TS_{1...n}$ of the test-suite is executed against corresponding version $V_{1...n}$ of the AUT and the behavioral state models for the test-suites (\textit{$TS_{M_1}, ...TS_{M_n}$}) are recorded. Each state model is uniquely identified by the corresponding \texttt{session-Id}. 

\item Afterwards, the behavioral state models are compared in a pair-wise form, i.e. \textit{$TS_{M_1} - TS_{M_2}$}, \textit{$TS_{M_2} - TS_{M_3}$}, ...\textit{$TS_{M_{n-1}} - TS_{M_n}$}.  

\item For comparing the each pair of behavioral models \textit{$TS_{M_{n-1}} - TS_{M_n}$}, the first step is to extract the maintenance metrics from both models 

\item In the second step, for each metric the change-sets are computed by comparing the tuple \texttt{action {\{key="value"}\}} for each action type. A change-set stores the pair of metric (i.e. action type) and corresponding value. Using this set, the number of repaired metric is output. 

\item In this manner, a change-set is computed for each maintenance metric presented in Table \ref{rq3metrics}.
% \end{itemize}


\end{enumerate}

Note that it is also possible to extract these metrics (e.g. GUI element locator requests) through the static analysis of the test-scripts or by means of checking the `diff' sets from the version control repository of these tests. However, such an analysis can be complicated in case of test-suites which implement the page-object pattern, since the metrics such as the GUI element locators can be encapsulated inside the page-objects. As a result, these metrics may not be available inside the test-scripts and one would have to sift through the page-objects as well. 

Additionally, in cases where only a subset of test-suite is to be tested, finding which page-objects are used for which test-scripts involves additional  human effort. On the other hand, with the current implementation, the behavior models are analyzed automatically to extract the metrics. In order to extract these metrics and implement the above-mentioned approach for answering RQ1, RQ2 and RQ3, a set of scripts has been written in programming language Python. 

% \end{textbf}