\chapter{Related Work} % Main chapter title

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 3. \emph{Related Work}}

Selenium tests might not be fully robust if they cover the intended behavior for the reference version but fail to cover the same behavior for subsequent test versions. Test failures can be detected by using assertions in the code, analyzing test reports, verifying that the application is in the intended state by capturing the screen-shots, finite state models for the application state etc. While useful, these techniques suffer certain drawbacks such as investment of time as well as manual efforts required for the analysis of their outcome. Approaches involving screenshot comparison might not be able to fully reveal the covered behavior and unexplored functionalities. Although finite state models can detect the covered functionalities, current implementations do not evaluate and compare these models for functional changes across different versions of an application. 

In the area of leveraging existing Selenium tests and generating behavioral models, the 'Testilizer' project by Mesbah et al \cite{testilizer} implements the combination of manual and automated test generation by leveraging manual tests with automated crawling. This approach focuses on mining the human-written tests to explore additional application functionality which might be unexplored by the crawler. It implements strict automatic assertion generation and as a result, the exploration depth to cover available functionality is limited. Another approach by Mesbah and Prasad \cite{CBCMesbah} proposes behavioral and functional differential analysis in the area of cross-browser compatibility; however it inherits the drawbacks suffered by 'Testilizer' such as limited recursion depth and additional effort required to identify active GUI elements. Another approach to cross-browser testing involves ‘WebDiff’ by Choudhary et al \cite{WebDiff} which implements DOM-tree matching and pair wise screen-shot comparison to detect cross browser issues. Our approach leverages the principles behind cross-browser analysis and applies the techniques such as functional and GUI-level differential analysis to differentiate the behavioral coverage achieved by Selenium tests across different versions of the AUT.

As a comparative study of maintainability of Selenium WebDriver test-suites, Leotta et al \cite{leotta2013comparing} propose an approach to analyze the effectiveness and failure of Selenium tests on the basis of Element-locators. Their approach limits the analysis to a comparative study of element locators and does not consider other factors mentioned in Section \ref{sec:RobustnessFactors} responsible for the instability of Selenium tests. 

Considering the current state-of-the-art technologies to our knowledge, there are no similar approaches to implement the robustness analysis of Selenium tests. Moreover, none of the aforementioned approaches evaluates the robustness of Selenium test-suites over the version history of the AUT. In contrast, our approach incorporates the existing techniques such as behavioral state models and GUI level comparisons to evaluate the robustness of Selenium tests and investigates the test failures across different regression cycles.